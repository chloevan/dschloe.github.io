<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Data Science | DSChloe</title>
    <link>https://dschloe.github.io/tags/spark/</link>
    <description>Recent content in Spark on Data Science | DSChloe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Sep 2023 10:00:30 +0900</lastBuildDate><atom:link href="https://dschloe.github.io/tags/spark/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark Installation with GCP (Sept. 2023)</title>
      <link>https://dschloe.github.io/gcp/2023/09/spark_installation_gcp/</link>
      <pubDate>Tue, 19 Sep 2023 10:00:30 +0900</pubDate>
      
      <guid>https://dschloe.github.io/gcp/2023/09/spark_installation_gcp/</guid>
      <description>개요 Spark를 구글 클라우드에 설치하도록 한다. 프로젝트 시작부터 진행한다. 프로젝트 시작 본 화면에서 새로운 프로젝트를 시작한다. 프로젝트명 : mulcampFP VM 시작하기 VM 만들기를 클릭한다. 활성 결제 계정이 없으면 결제계정을 등록한다. 결제계정이 등록되면 다음과 같이 화면이 나오면 VM 설정이 가능하다. 결제계정까지 완료가 되었으면 다음과 같이 Compute Engine API를 사용 버튼을 클릭해준다. 이름은 mulcamp-gcp 지역은 서울로 했다. 비용에 따라 성능을 선택할 수 있다. 호스트 유지보수 시, VM 인스턴스는 마이그레이션을 권장한다. 부팅 디스크는 Ubuntu로 변경했다.</description>
    </item>
    
    <item>
      <title>Spark Tutorial - Web UI on WSL</title>
      <link>https://dschloe.github.io/settings/spark_tutorial_web_ui/</link>
      <pubDate>Thu, 21 Apr 2022 12:30:47 +0900</pubDate>
      
      <guid>https://dschloe.github.io/settings/spark_tutorial_web_ui/</guid>
      <description>개요 간단하게 Spark Tutorial을 활용하여 Web UI를 가동한다. Spark Submit을 활용한다. 파이썬 가상환경 파이썬 가상환경을 작성한다. (필자의 경로는 pyskt_tutorial) $ pwd /mnt/c/hadoop/pyskt_tutorial 가상환경을 생성한다. evan@evan:/mnt/c/hadoop/pyskt_tutorial$ virtualenv venv 생성된 가상환경에 접속한다. evan@evan:/mnt/c/hadoop/pyskt_tutorial$ source venv/bin/activate (venv) evan@evan:/mnt/c/hadoop/pyskt_tutorial$ PySpark 설치 pyspark를 설치한다. (venv) evan@evan:/mnt/c/hadoop/pyskt_tutorial$ pip install pyspark Requirement already satisfied: pyspark in ./venv/lib/python3.8/site-packages (3.2.1) Requirement already satisfied: py4j==0.10.9.3 in ./venv/lib/python3.8/site-packages (from pyspark) (0.10.9.3) 데이터 생성 가상의 데이터를 생성한다. 소스파일과 구분 위해 data 폴더를 만든 후, 마크다운 파일을 하나 만들 것이다.</description>
    </item>
    
    <item>
      <title>WSL2에서의 Spark 설치</title>
      <link>https://dschloe.github.io/settings/spark_install_using_wsl/</link>
      <pubDate>Tue, 19 Apr 2022 12:30:47 +0900</pubDate>
      
      <guid>https://dschloe.github.io/settings/spark_install_using_wsl/</guid>
      <description>개요 간단하게 PySpark를 설치해보는 과정을 작성한다. WSL2 설치 방법은 다루지 않는다. 필수 파일 설치 자바 및 Spark 파일을 설치하도록 한다. $ sudo apt-get install openjdk-8-jdk $ sudo wget https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz $ sudo tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz .bashrc 파일 수정 필자의 현재 경로는 다음과 같다. evan@evan:/mnt/c/hadoop$ pwd /mnt/c/hadoop 설치한 파일은 다음과 같다. evan@evan:/mnt/c/hadoop$ ls spark-3.2.0-bin-hadoop3.2 spark-3.2.0-bin-hadoop3.2.tgz vi ~/.bashrc 파일을 열고 다음과 같이 코드를 작성한다. 다른 코드는 만지지 않는다. 가장 맨 마지막으로 내려온다. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export SPARK_HOME=/mnt/c/hadoop/spark-3.</description>
    </item>
    
    <item>
      <title>Spark Installation on M1 Mac</title>
      <link>https://dschloe.github.io/python/python_edu/00_settings/spark_installation_on_m1_mac/</link>
      <pubDate>Wed, 05 Jan 2022 14:00:47 +0900</pubDate>
      
      <guid>https://dschloe.github.io/python/python_edu/00_settings/spark_installation_on_m1_mac/</guid>
      <description>사전준비 M1 Mac에서 스파크를 설치하는 과정을 소개 하려고 한다. 필자의 Python 버전은 아래와 같다. $ python --version Python 3.8.7 자바 설치 자바 설치는 아래에서 다운로드 받았다. URL: Java SE Development Kit 8u301 그 다음 자바 설치를 확정한다. $ java --showversion 만약 에러가 아래와 같은 에러가 발생한다면, 시스템 환경설정 - Java - 업데이트 항목을 순차적으로 클릭한다. $ java --showversion Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred.</description>
    </item>
    
    <item>
      <title>Spark Installation on Windows 10</title>
      <link>https://dschloe.github.io/python/python_edu/00_settings/spark_installation_windows_10/</link>
      <pubDate>Mon, 03 Jan 2022 14:00:47 +0900</pubDate>
      
      <guid>https://dschloe.github.io/python/python_edu/00_settings/spark_installation_windows_10/</guid>
      <description>사전준비 스파크를 설치하는 과정은 소개 하려고 한다. 사전에 파이썬 3만 설치가 되어 있으면 된다. 만약, 파이썬이 처음이라면 Anaconda를 설치한다. 다운로드 전 필수 확인사항 스파크 설치 전에는 반드시 체크해야 하는 사항이 있다. (System Compatibility) 2022년 1월 기준은 아래와 같다. Get Spark from the downloads page of the project website. This documentation is for Spark version 3.2.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions.</description>
    </item>
    
  </channel>
</rss>
